{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a collection of python examples for \n",
    "# Numerical Optimization (Prof. Dr. Volker Schulz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 2: SVM example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib\n",
    "# from scikit learn documentation\n",
    "#\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs, make_classification\n",
    "\n",
    "\n",
    "# we create 50 separable points in blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=3)\n",
    "\n",
    "# fit the model, don't regularize for illustration purposes\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "# C >= 1 for hard margin\n",
    "# C = 0.01 for soft margin\n",
    "\n",
    "# create not nicely separable points\n",
    "#X, y = make_classification(n_samples=50,n_features=2, n_redundant=0, n_informative=2,\n",
    "#                            n_clusters_per_class=1,random_state=10)\n",
    "#clf = svm.SVC(kernel='rbf', C=1)\n",
    "# choose from ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, \n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "# after being fitted, the model can then be used to predict new values:\n",
    "print(clf.predict([[-6, -2]]))\n",
    "print(clf.predict([[-2, -1]]))\n",
    "print(clf.predict([[2, 1]])) \n",
    "print(clf.predict([[5, 6]])) \n",
    "\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "\n",
    "# plot the decision function\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 50)\n",
    "yy = np.linspace(ylim[0], ylim[1], 50)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "# plot support vectors\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chapter 2: 3D pic for SVM illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "s2=np.sqrt(2.0)\n",
    "\n",
    "L, n = 2.5, 400\n",
    "x = np.linspace(-L, L, n)\n",
    "y = x.copy()\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "# Compute random points\n",
    "N = 100\n",
    "rho = np.random.rand(N)\n",
    "phi = 2 * np.pi * np.random.rand(N)\n",
    "xb = rho * np.cos(phi)\n",
    "yb = rho * np.sin(phi)\n",
    "rho = np.random.rand(N)+1.5\n",
    "phi = 2 * np.pi * np.random.rand(N)\n",
    "xr = rho * np.cos(phi)\n",
    "yr = rho * np.sin(phi)\n",
    "\n",
    "\n",
    "# Set up a figure twice as wide as it is tall\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(xb, yb, 'bo',\n",
    "        xr, yr, 'ro')\n",
    "ax.grid(True)\n",
    "ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "surf = ax.plot_surface(X*X, Y*Y, s2*X*Y, rstride=20, cstride=20, cmap=cm.hot, alpha=0.25)\n",
    "ax.scatter(xb*xb, yb*yb, s2*xb*yb, color = 'blue')\n",
    "ax.scatter(xr*xr, yr*yr, s2*xr*yr, color = 'red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chapter 3:  illustration of convergence speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Max = 200\n",
    "t = range(1, Max)\n",
    "a = 0.9**(power(2,t))        # quadratic       red\n",
    "b=ones(size(t))\n",
    "b[0]=0.9\n",
    "for k in range(1, Max-1):\n",
    "    b[k] = b[k-1]/sqrt(k)    # superlinear     blue\n",
    "c = power(0.9,t)             # linear          green\n",
    "d = 0.9/(power(t,2))         # sublinear 2     cyan \n",
    "e = [0.9/k for k in t]       # sublinear 1     magenta\n",
    "f = 0.9/sqrt(t)              # sublinear 1/2   black\n",
    "\n",
    "#plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(10**(-12), 1)\n",
    "plt.plot(t, a, 'r') \n",
    "plt.plot(t, b, 'b')  \n",
    "plt.plot(t, c, 'g')  \n",
    "plt.plot(t, d, 'c')  \n",
    "plt.plot(t, e, 'm')  \n",
    "plt.plot(t, f, 'k') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chapter 3: [from steepest descent to CG](From_SD_to_CG.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chapter 3: simple CG implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import cg\n",
    "import time\n",
    "\n",
    "\n",
    "def conjugate_grad(Q, b, x=None):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    ----------\n",
    "    Solve a linear-quadratic problem min 1/2 x^TQx-b^Tx with conjugate gradient method with Q spd.\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q: 2d numpy.array of positive semi-definite (symmetric) matrix\n",
    "    b: 1d numpy.array\n",
    "    x: 1d numpy.array of initial point\n",
    "    Returns\n",
    "    -------\n",
    "    1d numpy.array x such that Qx = b\n",
    "    \"\"\"\n",
    "    n = len(b)\n",
    "    if not x:\n",
    "        x = np.ones(n)\n",
    "    r = np.dot(Q, x) - b\n",
    "    p = - r\n",
    "    r_k_norm = np.dot(r, r)\n",
    "    for i in range(2*n):\n",
    "        Qp = np.dot(Q, p) # can be replaced by a function evaluation of Q times p\n",
    "        alpha = r_k_norm / np.dot(p, Qp)\n",
    "        x += alpha * p\n",
    "        r += alpha * Qp\n",
    "        r_kplus1_norm = np.dot(r, r)\n",
    "        beta = r_kplus1_norm / r_k_norm\n",
    "        r_k_norm = r_kplus1_norm\n",
    "        if r_kplus1_norm < 1e-8:\n",
    "            print ('Itr:', i)\n",
    "            break\n",
    "        p = beta * p - r\n",
    "    return x\n",
    "\n",
    "\n",
    "def prec_conjugate_grad(Q, W, b, x=None):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    ----------\n",
    "    Solve a linear-quadratic problem min 1/2 x^TQx-b^Tx with preconditioned conjugate gradient method with Q spd.\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q: 2d numpy.array of positive semi-definite (symmetric) matrix\n",
    "    W: 2d numpy.array preconditioning matrix\n",
    "    b: 1d numpy.array\n",
    "    x: 1d numpy.array of initial point\n",
    "    Returns\n",
    "    -------\n",
    "    1d numpy.array x such that Qx = b\n",
    "    \"\"\"\n",
    "    n = len(b)\n",
    "    if not x:\n",
    "        x = np.ones(n)\n",
    "    r = np.dot(Q, x) - b\n",
    "    z = np.dot(W, r)\n",
    "    p = - z\n",
    "    r_k_z = np.dot(r, z)\n",
    "    for i in range(2*n):\n",
    "        Qp = np.dot(Q, p)\n",
    "        alpha = r_k_z / np.dot(p, Qp)\n",
    "        x += alpha * p\n",
    "        r += alpha * Qp\n",
    "        z = np.dot(W, r)\n",
    "        r_k_z_plus1 = np.dot(r, z)\n",
    "        beta = r_k_z_plus1 / r_k_z\n",
    "        r_k_z = r_k_z_plus1\n",
    "        if np.dot(r, r) < 1e-8:\n",
    "            print ('Itr:', i)\n",
    "            break\n",
    "        p = beta * p - z\n",
    "    return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n =1000\n",
    "    P = np.random.normal(size=[n, n])\n",
    "    Q = np.dot(P.T, P)\n",
    "    Q += 10000.0*np.diag(np.diag(Q))\n",
    "    b = np.ones(n)\n",
    "\n",
    "    t1 = time.time()\n",
    "    print ('start')\n",
    "    x = conjugate_grad(Q, b)\n",
    "    t2 = time.time()\n",
    "    print (t2 - t1)\n",
    "    x2 = np.linalg.solve(Q, b)\n",
    "    t3 = time.time()\n",
    "    print (t3 - t2)\n",
    "    x3 = cg(Q, b)\n",
    "    t4 = time.time()\n",
    "    print (t4 - t3)\n",
    "    #W=np.identity(n)\n",
    "    W=np.linalg.inv(np.diag(np.diag(Q))) # Jacobi preconditioner\n",
    "    #W=np.linalg.inv(Q)\n",
    "    t4 = time.time()\n",
    "    x = prec_conjugate_grad(Q, W, b)\n",
    "    t5 = time.time()\n",
    "    print (t5 - t4)\n",
    "    print (np.linalg.norm(np.dot(Q, x) - b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chapter 3: use CG to solve 1D heat equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse and symmetric matrix:\n",
    "\n",
    "$A=\\left[\n",
    "\\begin{array}{c}\n",
    "-2F & F\\\\\n",
    "F & -2F & F \\\\\n",
    "  & \\ddots & \\ddots & \\ddots\\\\\n",
    "   &        &  F     & -2F  \n",
    "   \\end{array}\n",
    "\\right]\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse.linalg as ssl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def callback(xk):\n",
    "    global num_iters\n",
    "    num_iters += 1\n",
    "\n",
    "Nx=3000\n",
    "F=(Nx+1)*(Nx+1);A = np.zeros((Nx, Nx))\n",
    "for i in range(1, Nx-1):\n",
    "    A[i,i-1]=-F;A[i,i]=2*F;A[i,i+1]=-F\n",
    "A[0,0]= 2*F; A[0,1]=-F\n",
    "A[Nx-1,Nx-1]= 2*F; A[Nx-1,Nx-2]= -F\n",
    "\n",
    "b=np.ones(Nx)\n",
    "x0=np.zeros(Nx)\n",
    "\n",
    "I=np.identity(Nx)\n",
    "A_upper_diag = np.triu(A, k=0)\n",
    "\n",
    "Aupinv= np.linalg.inv(A_upper_diag)\n",
    "M=0.5*(Aupinv+Aupinv.T)\n",
    "\n",
    "print(A)\n",
    "#print(np.linalg.eigvalsh(A))\n",
    "\n",
    "print(A_upper_diag)\n",
    "\n",
    "#with efficient incomplete LU (ilu) preconditioning\n",
    "num_iters = 0\n",
    "A_x = lambda x: A@x         # python lambda function used\n",
    "Aop = ssl.LinearOperator((Nx, Nx), A_x)\n",
    "Milu = ssl.spilu(A,options=dict(SymmetricMode=True))\n",
    "M_x = lambda x: Milu.solve(x) # python lambda function used\n",
    "Mop = ssl.LinearOperator((Nx, Nx), M_x)\n",
    "x, info = ssl.cg(Aop, b, x0=x0, M=Mop, maxiter=2000, tol=1e-5,callback=callback)\n",
    "print(\"# cg iter's with efficient ILU preconditioning\")\n",
    "print(num_iters)\n",
    "\n",
    "#with simple preconditioning\n",
    "num_iters = 0\n",
    "x, info = ssl.cg(A, b, x0=x0, M=M, maxiter=2000, tol=1e-5,callback=callback)\n",
    "print(\"# cg iter's with simple preconditioning\")\n",
    "print(num_iters)\n",
    "\n",
    "#without preconditioning\n",
    "num_iters = 0\n",
    "x, info = ssl.cg(A, b, x0=x0, M=I, maxiter=2000, tol=1e-5,callback=callback)\n",
    "print(\"# cg iter's without preconditioning\")\n",
    "print(num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Newton vs steepest descent (vs Quasi-Newton [comp_QN = 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import scipy.optimize as sopt\n",
    "\n",
    "import matplotlib.pyplot as pt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# choose problem type from\n",
    "# quadratic, rosenbrock\n",
    "problem = \"rosenbrock\"\n",
    "\n",
    "#choose step from\n",
    "# steepest, Newton \n",
    "step = \"Newton\"\n",
    "\n",
    "#choose comparison with quasi Newton\n",
    "# steepest, Newton \n",
    "comp_QN = 1 # 1: switches on\n",
    "\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    if problem == \"quadratic\":\n",
    "        return 0.5*x[0]**2 + 2.5*x[1]**2\n",
    "    elif problem == \"rosenbrock\":\n",
    "        return (1.0-x[0])**2 + 100.0*(x[1]-x[0]**2)**2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def df(x):\n",
    "    if problem == \"quadratic\":\n",
    "        return np.array([x[0], 5*x[1]])\n",
    "    elif problem == \"rosenbrock\":\n",
    "        return np.array([-2.0*(1.0-x[0]) - 400.0*(x[1]-x[0]**2)*x[0], 200.0*(x[1]-x[0]**2)])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def f1d(alpha):\n",
    "    return f(x + alpha*s)\n",
    "\n",
    "def quad_hess(x):\n",
    "     H = np.array([[1.0,0.0],[0.0,5.0]])\n",
    "     return H\n",
    "\n",
    "def rosen_hess(x):\n",
    "     x = np.asarray(x)\n",
    "     H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n",
    "     diagonal = np.zeros_like(x)\n",
    "     diagonal[0] = 1200*x[0]**2-400*x[1]+2\n",
    "     diagonal[-1] = 200\n",
    "     diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n",
    "     H = H + np.diag(diagonal)\n",
    "     return H\n",
    "\n",
    "def ddf(x):\n",
    "    if problem == \"quadratic\":\n",
    "        return quad_hess(x)\n",
    "    elif problem == \"rosenbrock\":\n",
    "        return rosen_hess(x)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def callback(xk):\n",
    "    guesses.append(xk)\n",
    "    return 0\n",
    "\n",
    "# Set up a figure twice as wide as it is tall\n",
    "fig = pt.figure(figsize=pt.figaspect(0.5))\n",
    "\n",
    "# Set up two subplots: left for 3d view, right for contour\n",
    "ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "\n",
    "xmesh, ymesh = np.mgrid[-2:2:75j,-2:2:75j]\n",
    "fmesh = f(np.array([xmesh, ymesh]))\n",
    "ax.plot_surface(xmesh, ymesh, fmesh)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "pt.axis(\"equal\")\n",
    "pt.contour(xmesh, ymesh, fmesh)\n",
    "\n",
    "guesses = [np.array([2, 2])] # 2,3./5\n",
    "\n",
    "x = guesses[-1]\n",
    "\n",
    "if step == \"steepest\":\n",
    "    maxit = 2000\n",
    "elif step == \"Newton\":\n",
    "    maxit = 15\n",
    "else:\n",
    "    print(\"choose optimization method\")\n",
    "\n",
    "\n",
    "\n",
    "for i in range(maxit):\n",
    "    s = -df(x)\n",
    "    if step == \"Newton\":\n",
    "        H=ddf(x)\n",
    "        s=la.solve(H,s)\n",
    "    if np.linalg.norm(s) < 0.0001:\n",
    "        break # step length as stopping criterion\n",
    "    alpha_opt = sopt.golden(f1d) #line search\n",
    "\n",
    "    x = x + alpha_opt * s\n",
    "    next_guess = x\n",
    "    guesses.append(next_guess)\n",
    "#    print(next_guess)\n",
    "\n",
    "print(\"number of optimization steps:\", i)\n",
    "pt.axis(\"equal\")\n",
    "pt.contour(xmesh, ymesh, fmesh, 150)\n",
    "it_array = np.array(guesses)\n",
    "pt.plot(it_array.T[0], it_array.T[1], \"x-\")\n",
    "\n",
    "#compare with L-BFGS Quasi-Newton\n",
    "if comp_QN == 1:\n",
    "    guesses = [np.array([2, 2])] # 2,3./5\n",
    "    x = guesses[-1]\n",
    "    sopt.minimize(f, x, method=\"L-BFGS-B\", jac=df, tol=0.00001, callback=callback, options={'maxcor': 2, 'maxiter': 100})\n",
    "    # c.f. https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "    it_array = np.array(guesses)\n",
    "    pt.plot(it_array.T[0], it_array.T[1], \"x-\")\n",
    "    print(\"number of QN steps:\", it_array.shape[0])\n",
    "\n",
    "pt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Rosenbrock as nonlinear least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "def fun_rosenbrock(x):\n",
    "    return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\n",
    "\n",
    "def jac_rosenbrock(x):\n",
    "    return np.array([\n",
    "        [-20 * x[0], 10],\n",
    "        [-1, 0]])\n",
    "\n",
    "x0_rosenbrock = np.array([2, 2])\n",
    "\n",
    "res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock)\n",
    "# c.f. https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares\n",
    "print('solution: ',res_2.x)\n",
    "print('final objective: ',res_2.cost)\n",
    "print('final norm of grad: ',res_2.optimality)\n",
    "print('# fcn eval.: ',res_2.nfev)\n",
    "print('# jac eval.: ',res_2.njev)\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Alternating least squares for explicit collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data from http://files.grouplens.org/datasets/movielens/\n",
    "# program from https://github.com/danielnee/Notebooks/blob/master/ALS/ALS_Explicit.ipynb\n",
    "# see also http://danielnee.com/2016/09/collaborative-filtering-using-alternating-least-squares/\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SimpleExample = True # False\n",
    "\n",
    "ratingsNames = [\"userId\", \"movieId\", \"rating\", \"timestamp\"]\n",
    "usersNames = [\"userId\", \"gender\", \"age\", \"occupation\", \"zipCode\"]\n",
    "moviesNames = [\"movieId\", \"title\", \"genres\"]\n",
    "\n",
    "if SimpleExample:\n",
    "    ratings = pd.read_table(\"simple-exp/rating1.dat\", header=None, sep=\"::\", names=ratingsNames)\n",
    "    users = pd.read_table(\"simple-exp/user1.dat\", header=None, sep=\"::\", names=usersNames)\n",
    "    movies = pd.read_table(\"simple-exp/mov1.dat\", header=None, sep=\"::\", names=moviesNames)\n",
    "    f=3; iters = 100\n",
    "    regLamba = 0.01\n",
    "else:\n",
    "    ratings = pd.read_table(\"ml-1m/ratings.dat\", header=None, sep=\"::\", names=ratingsNames)\n",
    "    users = pd.read_table(\"ml-1m/users.dat\", header=None, sep=\"::\", names=usersNames)\n",
    "    movies = pd.read_table(\"ml-1m/movies.dat\", header=None, sep=\"::\", names=moviesNames)\n",
    "    f=20; iters = 10\n",
    "    regLamba = 0.1\n",
    "\n",
    "print(movies.head())\n",
    "\n",
    "n = max(movies.movieId)\n",
    "m = max(users.userId)\n",
    "\n",
    "def normaliseRow(x):\n",
    "    return x / sum(x)\n",
    "\n",
    "def initialiseMatrix(n, f):\n",
    "    A = abs(np.random.randn(n, f))\n",
    "    return np.apply_along_axis(normaliseRow, 1, A)\n",
    "\n",
    "# Initialise Y matrix, n x f\n",
    "Y = initialiseMatrix(n, f)\n",
    "# Initialise X matrix, m x f\n",
    "X = initialiseMatrix(m, f)\n",
    "\n",
    "# Create a dummy entry for each movie\n",
    "temp = np.zeros((n, 4))\n",
    "for i in range(1, n):\n",
    "    temp[i,] = [m+1, i, 0, 0]\n",
    "    \n",
    "ratings = ratings.append(pd.DataFrame(temp, columns =ratingsNames))\n",
    "\n",
    "ratingsMatrix = ratings.pivot_table(columns=['movieId'], index =['userId'], values='rating', dropna = False)\n",
    "\n",
    "ratingsMatrix = ratingsMatrix.fillna(0).values  #ratingsMatrix.fillna(0).as_matrix() deprecated\n",
    "\n",
    "# Drop the dummy movie\n",
    "ratingsMatrix = ratingsMatrix[1:m+1,1:n+1]\n",
    "\n",
    "def ratingsPred(X, Y):\n",
    "    return np.dot(X, Y.T)\n",
    "\n",
    "def MSE(ratingsPred, ratingsMatrix):\n",
    "    idx = ratingsMatrix > 0\n",
    "    return sum((ratingsPred[idx] - ratingsMatrix[idx]) ** 2) / np.count_nonzero(ratingsMatrix)\n",
    "    \n",
    "print(MSE(ratingsPred(X, Y), ratingsMatrix))\n",
    "\n",
    "nonZero = ratingsMatrix > 0\n",
    "\n",
    "reg = regLamba * np.eye(f,f)\n",
    "\n",
    "for k in range(1, iters):\n",
    "    for i in range(1, m):\n",
    "        idx = nonZero[i,:]\n",
    "        a = Y[idx,]\n",
    "        b = np.dot(np.transpose(Y[idx,]), ratingsMatrix[i, idx])\n",
    "        updateX = np.linalg.solve(np.dot(np.transpose(a), a) + reg, b)\n",
    "        X[i,] = updateX\n",
    "    \n",
    "    for j in range(1, n):\n",
    "        idx = nonZero[:,j]\n",
    "        a = X[idx,]\n",
    "        b = np.dot(np.transpose(X[idx,]), ratingsMatrix[idx, j])\n",
    "        updateY = np.linalg.solve(np.dot(np.transpose(a), a) + reg, b)\n",
    "        Y[j,] = updateY\n",
    "        \n",
    "    ratingsP = ratingsPred(X, Y)\n",
    "    mse = MSE(ratingsP, ratingsMatrix)\n",
    "    print(\"MSE: \" + str(mse))\n",
    "    \n",
    "if SimpleExample:\n",
    "    print(ratingsP)        \n",
    "    print(ratingsMatrix)        \n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Examples of artificial neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation of \n",
    "# tensor flow for beginners from https://www.tensorflow.org/tutorials/quickstart/beginner \n",
    "Fashion_MNIST = True # False\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "\n",
    "# your python installation has to be enhanced by tensorflow 2 using one of the two methods:\n",
    "# a) pip install tensorflow      -> if you want to use your standard CPU, mostly done here\n",
    "# b) pip install tensorflow-gpu  -> if you want to use your GPU, should be nVidia with CUDA compatibility >= 3.5\n",
    "print(\"TensorFlow version: \",tf.__version__)\n",
    "\n",
    "# Load and prepare the MNIST dataset. Convert the samples from integers to floating-point numbers\n",
    "mnist = tf.keras.datasets.mnist\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "if Fashion_MNIST:\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "else:\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "if Fashion_MNIST:\n",
    "    x_test_store = x_test\n",
    "\n",
    "# summarize loaded dataset\n",
    "print('\\n Train size: X=%s, y=%s' % (x_train.shape, y_train.shape))\n",
    "print(' Test  size: X=%s, y=%s' % (x_test.shape, y_test.shape))\n",
    "# plot first few train images\n",
    "print(\"\\n The first 9 training pictures; \")\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "    pyplot.imshow(x_train[i], cmap=pyplot.get_cmap('gray'))\n",
    "# show the figure\n",
    "pyplot.show()\n",
    "\n",
    "if not Fashion_MNIST:\n",
    "# plot first few test images\n",
    "    print(\"\\n The first 9 test pictures: \")\n",
    "    for i in range(9):\n",
    "        # define subplot\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        # plot raw pixel data\n",
    "        pyplot.imshow(x_test[i], cmap=pyplot.get_cmap('gray'))\n",
    "    # show the figure\n",
    "    pyplot.show()\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "\n",
    "#Build the tf.keras.Sequential model by stacking layers. \n",
    "# find more layers on https://keras.io/api/layers/\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10) # defaul activation None, i.e. linear\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "#For each example the model returns a vector of \"logits\" or \"log-odds\" scores, one for each class.\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "print(\"\\n Prediction vector example for first training picture\")\n",
    "print(predictions)\n",
    "\n",
    "#The tf.nn.softmax function converts these logits to \"probabilities\" for each class:\n",
    "print(\"\\n Probability vector example for first training picture\")\n",
    "print(tf.nn.softmax(predictions).numpy())\n",
    "\n",
    "#Choose a loss function for training\n",
    "#The losses.SparseCategoricalCrossentropy loss takes a vector of logits and a True index and returns a scalar loss for each example.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "#This loss is equal to the negative log probability of the true class: It is zero if the model is sure of the correct class.\n",
    "#This untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to -tf.log(1/10) ~= 2.3.\n",
    "print(\"\\n Loss example for first training picture\")\n",
    "print(loss_fn(y_train[:1], predictions).numpy())\n",
    "\n",
    "#Choose an optimizer \n",
    "#for default values, see https://keras.io/api/optimizers/\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#The Model.fit method adjusts the model parameters to minimize the loss:\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "#The Model.evaluate method checks the models performance, usually on a \"Validation-set\" or \"Test-set\".\n",
    "model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "#let us look at probabilities, wrap the trained model, and attach the softmax to it:\n",
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "\n",
    "if Fashion_MNIST:\n",
    "    eval_labels = tf.keras.backend.argmax(probability_model(x_test[:30]))\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    pyplot.figure(figsize=(10,10))\n",
    "    for i in range(25):\n",
    "        pyplot.subplot(5,5,i+1)\n",
    "        pyplot.xticks([])\n",
    "        pyplot.yticks([])\n",
    "        pyplot.grid(False)\n",
    "        pyplot.imshow(x_test_store[i], cmap=pyplot.cm.binary)\n",
    "        pyplot.xlabel(class_names[eval_labels[i]])\n",
    "    pyplot.show()\n",
    "    \n",
    "else:\n",
    "    #print(\"\\n Probability vectors for first 9 test images\")\n",
    "    #print(probability_model(x_test[:9]))\n",
    "    print(\"\\n Argmax vector for first 9 test images\")\n",
    "    print(tf.keras.backend.argmax(probability_model(x_test[:9])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: challenging neural network based on CNN for fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow_version 2.x\n",
    "# Hifi CNN with multiple layers\n",
    "# see for other NN architectures: https://github.com/zalandoresearch/fashion-mnist\n",
    "# use GPU, e.g. on https://colab.research.google.com/notebooks/gpu.ipynb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, BatchNormalization, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "\n",
    "def make_model():\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', input_shape=(28,28,1)))\n",
    "  model.add(BatchNormalization())\n",
    "\n",
    "  model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', strides=1, padding='same'))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(0.25))\n",
    "    \n",
    "  model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', strides=1, padding='same'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Dropout(0.25))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512, activation='relu'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(10, activation='softmax'))\n",
    "  return model\n",
    "\n",
    "model = make_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "print (train_images.shape)\n",
    "print (train_labels.shape)\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam (lr=0.001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\n",
    "\n",
    "#model.fit(train_images, train_labels, epochs=30, callbacks=[reduce_lr])\n",
    "model.fit(train_images, train_labels,\n",
    "          batch_size=100,\n",
    "          epochs=30,\n",
    "          callbacks=[reduce_lr],\n",
    "          verbose=1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5. Some QP Solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, dot\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from qpsolvers import solve_qp, available_solvers\n",
    "#sudo -H pip install qpsolvers  # comes with quadprog from Goldfarb\n",
    "#sudo -H pip install cvxopt  # interior point method\n",
    "#sudo -H pip install osqp  #admm\n",
    "#https://pypi.org/project/qpsolvers/\n",
    "\n",
    "#choose QP solver from: CVXOPT, qpOASES, quadprog, ECOS as wrapped by CVXPY, Gurobi, MOSEK, OSQP\n",
    "#QP in standard form according to https://scaron.info/teaching/quadratic-programming.html\n",
    "#\n",
    "\n",
    "M = array([[1., 2., 0.], [-8., 3., 2.], [0., 1., 1.]])\n",
    "P = dot(M.T, M)  # quick way to build a symmetric matrix\n",
    "q = dot(array([3., 2., 3.]), M).reshape((3,))\n",
    "G = array([[1., 2., 1.], [2., 0., 1.], [-1., 2., -1.]])\n",
    "h = array([3., 2., -2.]).reshape((3,))\n",
    "A = array([1., 1., 1.])\n",
    "b = array([1.])\n",
    "\n",
    "print(\"QP:\")\n",
    "display(Latex('$\\min {1\\over 2} x^T Px+q^T x$'))\n",
    "display(Latex('s.t. $Ax = b$'))\n",
    "display(Latex('and $Gx\\leq h$'))\n",
    "\n",
    "print(\"where:\")\n",
    "\n",
    "print(\"\\n P = \"), print(P)\n",
    "print(\"\\n q = \"), print(q)\n",
    "print(\"\\n A = \"), print(A)\n",
    "print(\"\\n b = \"), print(b)\n",
    "print(\"\\n G = \"), print(G)\n",
    "print(\"\\n h = \"), print(h)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "print(\"options for solvers: \", available_solvers)\n",
    "\n",
    "print(\"QP solution:\")\n",
    "print(\"x = \",solve_qp(P, q, G, h, A, b,solver=\"cvxopt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: Markowitz portfolio optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the portfolio Jupyter notebook from  https://www.quantopian.com/posts/a-tutorial-on-markowitz-portfolio-optimization-in-python-using-cvxopt\n",
    "    but updated to Python 3. There, you may find also a section on backtesting on real market data, which I omit here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxopt as opt\n",
    "from cvxopt import blas, solvers\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Turn off progress printing \n",
    "solvers.options['show_progress'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have 4 assets, each with a return series of length 1000. We can use numpy.random.randn to sample returns from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NUMBER OF ASSETS\n",
    "n_assets = 4\n",
    "\n",
    "## NUMBER OF OBSERVATIONS\n",
    "n_obs = 1000\n",
    "\n",
    "return_vec = np.random.randn(n_assets, n_obs)\n",
    "\n",
    "plt.plot(return_vec.T, alpha=.4);\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('returns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These return series can be used to create a wide range of portfolios, which all have different returns and risks (standard deviation). We can produce a wide range of random weight vectors and plot those portfolios. As we want all our capital to be invested, this vector will have to some to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_weights(n):\n",
    "    ''' Produces n random weights that sum to 1 '''\n",
    "    k = np.random.rand(n)\n",
    "    return k / sum(k)\n",
    "\n",
    "print(rand_weights(n_assets))\n",
    "print(rand_weights(n_assets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets evaluate how many of these random portfolios would perform. Towards this goal we are calculating the mean returns as well as the volatility (here we are using standard deviation). You can also see that there is a filter that only allows to plot portfolios with a standard deviation of < 2 for better illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_portfolio(returns):\n",
    "    ''' \n",
    "    Returns the mean and standard deviation of returns for a random portfolio\n",
    "    '''\n",
    "\n",
    "    p = np.asmatrix(np.mean(returns, axis=1))\n",
    "    w = np.asmatrix(rand_weights(returns.shape[0]))\n",
    "    C = np.asmatrix(np.cov(returns))\n",
    "    \n",
    "    mu = w * p.T\n",
    "    sigma = np.sqrt(w * C * w.T)\n",
    "    \n",
    "    # This recursion reduces outliers to keep plots pretty\n",
    "    if sigma > 2:\n",
    "        return random_portfolio(returns)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code you will notice the calculation of the return with:\n",
    "$$ R = p^T w $$\n",
    "where $R$ is the expected return, $p^T$ is the transpose of the vector for the mean returns for each time series and w is the weight vector of the portfolio. $p$ is a Nx1 column vector, so $p^T$ turns into a 1xN row vector which can be multiplied with the Nx1 weight (column) vector w to give a scalar result. This is equivalent to the dot product used in the code. Keep in mind that Python has a reversed definition of rows and columns and the accurate NumPy version of the previous equation would be R = w * p.T\n",
    "Next, we calculate the standard deviation with\n",
    "$$\\sigma = \\sqrt{w^T C w}$$\n",
    "where $C$ is the covariance matrix of the returns which is a NxN matrix. Please note that if we simply calculated the simple standard deviation with the appropriate weighting using std(array(ret_vec).T*w) we would get a slightly different ’bullet’. This is because the simple standard deviation calculation would not take covariances into account. In the covariance matrix, the values of the diagonal represent the simple variances of each asset while the off-diagonals are the variances between the assets. By using ordinary std() we effectively only regard the diagonal and miss the rest. A small but significant difference.\n",
    "Lets generate the mean returns and volatility for 500 random portfolios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_portfolios = 500\n",
    "means, stds = np.column_stack([\n",
    "    random_portfolio(return_vec) \n",
    "    for _ in range(n_portfolios)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon plotting those you will observe that they form a characteristic parabolic shape called the ‘Markowitz bullet‘ with the boundaries being called the ‘efficient frontier‘, where we have the lowest variance for a given expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stds, means, 'o', markersize=5)\n",
    "plt.xlabel('std')\n",
    "plt.ylabel('mean')\n",
    "plt.title('Mean and standard deviation of returns of randomly generated portfolios')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markowitz optimization and the Efficient Frontier\n",
    "\n",
    "Once we have a good representation of our portfolios as the blue dots show we can calculate the efficient frontier Markowitz-style. This is done by minimising\n",
    "$$ w^T C w$$\n",
    "for $w$ on the expected portfolio return $R^T w$ whilst keeping the sum of all the weights equal to 1:\n",
    "$$ \\sum_{i}{w_i} = 1 $$\n",
    "Here we parametrically run through $R^T w = \\mu$ and find the minimum variance for different $\\mu$‘s. This can be done with scipy.optimise.minimize but we have to define quite a complex problem with bounds, constraints and a Lagrange multiplier. Conveniently, the cvxopt package, a convex solver, does all of that for us. We used one of their examples with some modifications as shown below. You will notice that there are some conditioning expressions in the code. They are simply needed to set up the problem. For more information please have a look at the cvxopt example.\n",
    "The mus vector produces a series of expected return values $\\mu$ in a non-linear and more appropriate way. We will see later that we don‘t need to calculate a lot of these as they perfectly fit a parabola, which can safely be extrapolated for higher values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_portfolio(returns):\n",
    "    n = len(returns)\n",
    "    returns = np.asmatrix(returns)\n",
    "    \n",
    "    N = 100\n",
    "    mus = [10**(5.0 * t/N - 1.0) for t in range(N)]\n",
    "    \n",
    "    # Convert to cvxopt matrices\n",
    "    S = opt.matrix(np.cov(returns))\n",
    "    pbar = opt.matrix(np.mean(returns, axis=1))\n",
    "    \n",
    "    # Create constraint matrices\n",
    "    G = -opt.matrix(np.eye(n))   # negative n x n identity matrix\n",
    "    h = opt.matrix(0.0, (n ,1))\n",
    "    A = opt.matrix(1.0, (1, n))\n",
    "    b = opt.matrix(1.0)\n",
    "    \n",
    "    # Calculate efficient frontier weights using quadratic programming\n",
    "    portfolios = [solvers.qp(mu*S, -pbar, G, h, A, b)['x'] \n",
    "                  for mu in mus]\n",
    "    ## CALCULATE RISKS AND RETURNS FOR FRONTIER\n",
    "    returns = [blas.dot(pbar, x) for x in portfolios]\n",
    "    risks = [np.sqrt(blas.dot(x, S*x)) for x in portfolios]\n",
    "    ## CALCULATE THE 2ND DEGREE POLYNOMIAL OF THE FRONTIER CURVE\n",
    "    m1 = np.polyfit(returns, risks, 2)\n",
    "    x1 = np.sqrt(m1[2] / m1[0])\n",
    "    # CALCULATE THE OPTIMAL PORTFOLIO -> CALL the QP solver CVXOPT\n",
    "    wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)['x']\n",
    "    return np.asarray(wt), returns, risks\n",
    "\n",
    "weights, returns, risks = optimal_portfolio(return_vec)\n",
    "\n",
    "plt.plot(stds, means, 'o')\n",
    "plt.ylabel('mean')\n",
    "plt.xlabel('std')\n",
    "plt.plot(risks, returns, 'y-o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In yellow you can see the optimal portfolios for each of the desired returns (i.e. the mus). In addition, we get the one optimal portfolio returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Lasso with ADMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb,time\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from numpy.linalg import norm,cholesky\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/afbujan/admm_lasso\n",
    "Author  : Alex Bujan (adapted from http://www.stanford.edu/~boyd)\n",
    "Date    : 12/06/2015\n",
    "06/30/2020 : ported to Python 3 and slighty changed (Volker Schulz)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def lasso_admm(X,y,alpha,rho=1.,rel_par=1.,QUIET=False,\\\n",
    "                MAX_ITER=50,ABSTOL=1e-3,RELTOL= 1e-2):\n",
    "    \"\"\"\n",
    "     Solve lasso problem via ADMM\n",
    "    \n",
    "     [z, history] = lasso_admm(X,y,alpha,rho,rel_par)\n",
    "    \n",
    "     Solves the following problem via ADMM:\n",
    "    \n",
    "       minimize 1/2*|| Ax - y ||_2^2 + alpha || x ||_1\n",
    "    \n",
    "     The solution is returned in the vector z.\n",
    "    \n",
    "     history is a dictionary containing the objective value, the primal and\n",
    "     dual residual norms, and the tolerances for the primal and dual residual\n",
    "     norms at each iteration.\n",
    "    \n",
    "     rho is the augmented Lagrangian parameter.\n",
    "    \n",
    "     rel_par is the over-relaxation parameter (typical values for rel_par are\n",
    "     between 1.0 and 1.8).\n",
    "    \n",
    "     More information can be found in the paper linked at:\n",
    "     http://www.stanford.edu/~boyd/papers/distr_opt_stat_learning_admm.html\n",
    "    \"\"\"\n",
    "\n",
    "    if not QUIET:\n",
    "        tic = time.time()\n",
    "\n",
    "    #Data preprocessing\n",
    "    m,n = X.shape\n",
    "    #save a matrix-vector multiply\n",
    "    Xty = X.T.dot(y)\n",
    "\n",
    "    #ADMM solver\n",
    "    x = np.zeros((n,1))\n",
    "    z = np.zeros((n,1))\n",
    "    u = np.zeros((n,1))\n",
    "\n",
    "    # cache the (Cholesky) factorization\n",
    "    L,U = factor(X,rho)\n",
    "\n",
    "    if not QUIET:\n",
    "        print ('\\n%3s\\t%10s\\t%10s\\t%10s\\t%10s\\t%10s' % ('iter',\n",
    "                                                      'r norm', \n",
    "                                                      'eps pri', \n",
    "                                                      's norm', \n",
    "                                                      'eps dual', \n",
    "                                                      'objective'))\n",
    "\n",
    "    # Saving state\n",
    "    h = {}\n",
    "    h['objval']     = np.zeros(MAX_ITER)\n",
    "    h['r_norm']     = np.zeros(MAX_ITER)\n",
    "    h['s_norm']     = np.zeros(MAX_ITER)\n",
    "    h['eps_pri']    = np.zeros(MAX_ITER)\n",
    "    h['eps_dual']   = np.zeros(MAX_ITER)\n",
    "\n",
    "    for k in range(MAX_ITER):\n",
    "\n",
    "        # x-update \n",
    "        q = Xty+rho*(z-u) #(temporary value)\n",
    "        if m>=n:\n",
    "            x = spsolve(U,spsolve(L,q))[...,np.newaxis]\n",
    "        else:\n",
    "            ULXq = spsolve(U,spsolve(L,X.dot(q)))[...,np.newaxis]\n",
    "            x = (q*1./rho)-((X.T.dot(ULXq))*1./(rho**2))\n",
    "\n",
    "        # z-update with relaxation\n",
    "        zold = np.copy(z)\n",
    "        x_hat = rel_par*x+(1.-rel_par)*zold\n",
    "        z = shrinkage(x_hat+u,alpha*1./rho)\n",
    "\n",
    "        # u-update\n",
    "        u+=(x_hat-z)\n",
    "\n",
    "        # diagnostics, reporting, termination checks\n",
    "        h['objval'][k]   = objective(X,y,alpha,x,z)\n",
    "        h['r_norm'][k]   = norm(x-z)\n",
    "        h['s_norm'][k]   = norm(-rho*(z-zold))\n",
    "        h['eps_pri'][k]  = np.sqrt(n)*ABSTOL+\\\n",
    "                            RELTOL*np.maximum(norm(x),norm(-z))\n",
    "        h['eps_dual'][k] = np.sqrt(n)*ABSTOL+\\\n",
    "                            RELTOL*norm(rho*u)\n",
    "        if not QUIET:\n",
    "            print ('%4d\\t%10.4f\\t%10.4f\\t%10.4f\\t%10.4f\\t%10.2f' % (k+1,\\\n",
    "                                                          h['r_norm'][k],\\\n",
    "                                                          h['eps_pri'][k],\\\n",
    "                                                          h['s_norm'][k],\\\n",
    "                                                          h['eps_dual'][k],\\\n",
    "                                                          h['objval'][k]))\n",
    "\n",
    "        if (h['r_norm'][k]<h['eps_pri'][k]) and (h['s_norm'][k]<h['eps_dual'][k]) and (k>30):\n",
    "            break\n",
    "\n",
    "    if not QUIET:\n",
    "        toc = time.time()-tic\n",
    "        print (\"\\nElapsed time is %.2f seconds\" % toc)\n",
    "\n",
    "    return z.ravel(),h\n",
    "\n",
    "def objective(X,y,alpha,x,z):\n",
    "    return .5*np.square(X.dot(x)-y).sum()+alpha*norm(z,1)\n",
    "\n",
    "def shrinkage(x,kappa):\n",
    "    return np.maximum(0.,x-kappa)-np.maximum(0.,-x-kappa)\n",
    "\n",
    "def factor(X,rho):\n",
    "    m,n = X.shape\n",
    "    if m>=n:\n",
    "       L = cholesky(X.T.dot(X)+rho*sparse.eye(n))\n",
    "    else:\n",
    "       L = cholesky(sparse.eye(m)+1./rho*(X.dot(X.T)))\n",
    "    L = sparse.csc_matrix(L)\n",
    "    U = sparse.csc_matrix(L.T)\n",
    "    return L,U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sparse\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Author  : Alex Bujan (adapted from http://www.stanford.edu/~boyd)\n",
    "Date    : 12/06/2015\n",
    "06/30/2020 : ported to Python 3 and slighty changed (Volker Schulz)\n",
    "\"\"\"\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "m = 1500        # number of examples\n",
    "n = 5000        # number of features\n",
    "p = 100/n       # sparsity density\n",
    "\n",
    "x0  = sparse.rand(n,1,p)\n",
    "X   = np.random.randn(m,n)\n",
    "D   = sparse.diags(1./np.sqrt(np.sum(X**2,0)).T,0,(n,n))\n",
    "X   = D.T.dot(X.T).T\n",
    "y   = x0.T.dot(X.T).T+np.sqrt(1e-3)*np.random.randn(m,1)\n",
    "\n",
    "\n",
    "alpha_max = norm(X.T.dot(y),np.inf)\n",
    "alpha = .1*alpha_max\n",
    "\n",
    "print ('\\n*Alpha: %.4f' % alpha)\n",
    "\n",
    "x, h = lasso_admm(X,y,alpha,1.,1.)\n",
    "\n",
    "K = len(h['objval'][np.where(h['objval']!=0)])\n",
    "\n",
    "fig1 = plt.figure(1)\n",
    "ax = fig1.add_subplot(111)\n",
    "ax.plot(np.arange(K), h['objval'][:K],'k',ms=10,lw=2)\n",
    "ax.set_ylabel('f(x^k) + g(z^k)')\n",
    "ax.set_xlabel('iter (k)')\n",
    "\n",
    "fig2 = plt.figure(2)\n",
    "ax1 = fig2.add_subplot(211)\n",
    "ax1.semilogy(np.arange(K),np.maximum(1e-8,h['r_norm'][:K]),'k',lw=2)\n",
    "ax1.semilogy(np.arange(K),h['eps_pri'][:K],'k--',lw=2)\n",
    "ax1.set_ylabel('||r||_2')\n",
    "\n",
    "ax2 = fig2.add_subplot(212)\n",
    "ax2.semilogy(np.arange(K),np.maximum(1e-8,h['s_norm'][:K]),'k',lw=2)\n",
    "ax2.semilogy(np.arange(K),h['eps_dual'][:K],'k--',lw=2)\n",
    "ax2.set_ylabel('||s||_2')\n",
    "ax2.set_xlabel('iter (k)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: SQP for a simple problem\n",
    "see documentation in https://docs.scipy.org/doc/scipy-0.19.0/reference/tutorial/optimize.html#tutorial-sqlsp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# objective function\n",
    "def func(x, sign=1.0):\n",
    "    return sign*(2*x[0]*x[1] + 2*x[0] - x[0]**2 - 2*x[1]**2)\n",
    "\n",
    "#derivative of objective function\n",
    "def func_deriv(x, sign=1.0):\n",
    "    dfdx0 = sign*(-2*x[0] + 2*x[1] + 2)\n",
    "    dfdx1 = sign*(2*x[0] - 4*x[1])\n",
    "    return np.array([ dfdx0, dfdx1 ])\n",
    "    \n",
    "cons = ({'type': 'eq',\n",
    "          'fun' : lambda x: np.array([x[0]**3 - x[1]]),\n",
    "          'jac' : lambda x: np.array([3.0*(x[0]**2.0), -1.0])},\n",
    "         {'type': 'ineq', # >= 0\n",
    "          'fun' : lambda x: np.array([x[1] - 1]),\n",
    "          'jac' : lambda x: np.array([0.0, 1.0])})\n",
    "\n",
    "# unconstrained minimization\n",
    "res = minimize(func, [-1.0,1.0], args=(-1.0,), jac=func_deriv,\n",
    "                method='SLSQP', options={'disp': True})\n",
    "print(res.x)\n",
    "\n",
    "#constrained minimization\n",
    "res = minimize(func, [-1.0,1.0], args=(-1.0,), #jac=func_deriv,\n",
    "                constraints=cons, method='SLSQP', options={'disp': True})\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: SQP nonnegative low rank approximation of matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print('\\033[1m'+\"Explicit recommendation matrix [<=0 means no observation]\"+'\\033[0m')\n",
    "\n",
    "# define customer matrix from chapter 3.6\n",
    "A_goal = np.array([[4,1,1,4], [1,4,2,0],[2,1,4,5],[1,4,1,-0.1]])\n",
    "print(A_goal)\n",
    "m = A_goal.shape[0]\n",
    "n = A_goal.shape[1]\n",
    "\n",
    "k_rank = 2  # rank of the low rank approximation\n",
    "\n",
    "numvar = m*k_rank + k_rank*n\n",
    "\n",
    "x = np.random.rand(numvar,1)\n",
    "x = np.reshape(x,numvar) \n",
    "\n",
    "xrand = x\n",
    "\n",
    "# objective function\n",
    "def func(x):\n",
    "    regfac = 0.1 # regularitation factor, sometimes called lambda\n",
    "    x_split = np.split(x,[m*k_rank, numvar])\n",
    "    U = np.reshape(x_split[0],(n,k_rank))\n",
    "    V = np.reshape(x_split[1],(m,k_rank))\n",
    "    A = U@V.T\n",
    "    obj = 0.0\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if A_goal[i,j] >= 0.0 :\n",
    "                obj += (A_goal[i,j]-A[i,j])**2\n",
    "    obj += regfac*np.linalg.norm(U, 'fro')**2\n",
    "    obj += regfac*np.linalg.norm(V, 'fro')**2\n",
    "    return obj\n",
    "\n",
    "# unconstrained minimization\n",
    "print('\\033[1m'+\"\\n\"+\"Unconstrained optimization result:\"+'\\033[0m')\n",
    "\n",
    "res = minimize(func, x, method='SLSQP', options={'disp': True})\n",
    "\n",
    "x_split = np.split(res.x,[m*k_rank, numvar])\n",
    "recmat = np.reshape(x_split[0],(n,k_rank))@np.reshape(x_split[1],(m,k_rank)).T\n",
    "print(recmat)\n",
    "\n",
    "# constrained minimization\n",
    "\n",
    "print('\\033[1m'+\"\\n\"+\"Nonnegatively constrained optimization result:\"+'\\033[0m')\n",
    "\n",
    "def confunc(x):\n",
    "    x_split = np.split(x,[m*k_rank, numvar])\n",
    "    A = np.reshape(x_split[0],(n,k_rank))@np.reshape(x_split[1],(m,k_rank)).T\n",
    "    return np.reshape(A,m*n)\n",
    "\n",
    "cons = {'type': 'ineq', # >=0\n",
    "        'fun' : lambda x: confunc(x)}\n",
    "\n",
    "res = minimize(func, xrand, constraints=cons, method='SLSQP', options={'disp': True})\n",
    "\n",
    "x_split = np.split(res.x,[m*k_rank, numvar])\n",
    "recmat = np.reshape(x_split[0],(n,k_rank))@np.reshape(x_split[1],(m,k_rank)).T\n",
    "print(recmat)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
