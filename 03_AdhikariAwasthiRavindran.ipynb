{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Rosengerg function = (1 - x)**2 + 100*(-x**2 + y)**2\n",
      "______________________________________\n",
      "Gradient = [[-400*x*(-x**2 + y) + 2*x - 2]\n",
      " [-200*x**2 + 200*y]]\n",
      "______________________________________\n",
      "Hessian matrix = [[1200*x**2 - 400*y + 2 -400*x]\n",
      " [-400*x 200]]\n",
      "_______________________________________\n",
      "without any parameter\n",
      "for x0 = [0,0]\n",
      "     fun: 2.008766330886964e-11\n",
      "     jac: array([ 5.61301577e-06, -2.80553174e-06])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 165\n",
      "     nit: 21\n",
      "    njev: 55\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.99999552, 0.99999102])\n",
      "_______________________________________\n",
      "for x0 = [0.99,0.99]\n",
      "     fun: 8.774126958041593e-12\n",
      "     jac: array([ 1.06243383e-04, -4.69376313e-05])\n",
      " message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "    nfev: 221\n",
      "     nit: 14\n",
      "    njev: 70\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([1.00000171, 1.00000317])\n",
      "_______________________________________\n",
      "only jacobian\n",
      "for x0 = [0,0]\n",
      "     fun: 6.317906225223875e-11\n",
      "     jac: array([-6.80688609e-06, -4.54187409e-06])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 42\n",
      "     nit: 18\n",
      "    njev: 42\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.99999205, 0.99998409])\n",
      "_______________________________________\n",
      "for x0 = [0.99,0.99]\n",
      "     fun: 3.936924551588551e-14\n",
      "     jac: array([ 7.86822927e-06, -3.95206479e-06])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 28\n",
      "     nit: 9\n",
      "    njev: 28\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.99999998, 0.99999994])\n",
      "_______________________________________\n",
      "Newton-CG\n",
      "for x0 = [0,0]\n",
      "     fun: 3.949981563726443e-16\n",
      "     jac: array([ 2.80008950e-05, -1.44988923e-05])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 49\n",
      "    nhev: 0\n",
      "     nit: 31\n",
      "    njev: 118\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1.        , 1.00000001])\n",
      "_______________________________________\n",
      "for x0 = [0.99,0.99]\n",
      "     fun: 3.556179144831777e-12\n",
      "     jac: array([9.28987365e-06, 1.70220198e-06])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 6\n",
      "    nhev: 0\n",
      "     nit: 5\n",
      "    njev: 19\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1.00000188, 1.00000377])\n",
      "_______________________________________\n"
     ]
    }
   ],
   "source": [
    "#Saurav Adhikari - 1622912\n",
    "#Lalita Awasthi - 1622924\n",
    "#Nila Ravindran - 1614113\n",
    "\n",
    "#Importing necessary modules\n",
    "from scipy.optimize import optimize\n",
    "import numpy as np\n",
    "from sympy import diff,Symbol\n",
    "\n",
    "\n",
    "#Implementing Rosenberg function\n",
    "def Rosenberg():\n",
    "    x = Symbol('x')\n",
    "    y = Symbol('y')\n",
    "    z = (1-x)**2 + 100 * (y-x**2)**2\n",
    "    return z\n",
    "\n",
    "#Calculating the gradient of the Rosenberg function simply by generating the partial derivative\n",
    "def gradient(z):\n",
    "    x = Symbol('x')\n",
    "    y = Symbol('y')\n",
    "    gradient = np.array([[z.diff(x)],[z.diff(y)]])\n",
    "    return gradient\n",
    "\n",
    "#Calculating the Hessian matrix just like in the first exercise\n",
    "def Hess(grad):\n",
    "\n",
    "    x = Symbol('x')\n",
    "    y = Symbol('y')\n",
    "    x1= grad[0,0].diff(x)\n",
    "    x2= grad[0,0].diff(y)\n",
    "    x3 = grad[1,0].diff(x)\n",
    "    x4 = grad[1,0].diff(y)\n",
    "\n",
    "    hess = np.matrix([[x1,x2],[x3,x4]])\n",
    "    return hess\n",
    "\n",
    "#redefining the rosenberg function so that it will be easier to optimize\n",
    "def f (x):\n",
    "    return (1-x[0])**2 + 100 * (x[1] - x[0]**2) **2\n",
    "\n",
    "#jacobian function with the gradient value\n",
    "def jacobian(x):\n",
    "   return np.array((-400*x[0]*(-x[0] ** 2 + x[1])+ 2*x[0] -2 , -200 * x[0] **2 + 200*x[1]))\n",
    "\n",
    "#main method\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #Calling the rosenberg function\n",
    "    z = Rosenberg()\n",
    "    print(f' Rosengerg function = {z}')\n",
    "    print(\"______________________________________\")\n",
    "\n",
    "    #Calling the gradient method\n",
    "    grad = gradient(z)\n",
    "    print(f'Gradient = {grad}')\n",
    "    print(\"______________________________________\")\n",
    "\n",
    "    #Hessian Method\n",
    "    hessian = Hess(grad)\n",
    "    print(f'Hessian matrix = {hessian}')\n",
    "    print(\"_______________________________________\")\n",
    "\n",
    "    print(\"without any parameter\")\n",
    "    print('for x0 = [0,0]')\n",
    "    print(optimize._minimize_cg(f,[0,0]))\n",
    "    print(\"_______________________________________\")\n",
    "\n",
    "    print('for x0 = [0.99,0.99]')\n",
    "    print(optimize._minimize_cg(f,[0.99,0.99]))\n",
    "    print(\"_______________________________________\")\n",
    "\n",
    "    print('only jacobian')\n",
    "    print('for x0 = [0,0]')\n",
    "    print(optimize._minimize_cg(f,[0,0],jac=jacobian))\n",
    "    print(\"_______________________________________\")\n",
    "\n",
    "    print('for x0 = [0.99,0.99]')\n",
    "    print(optimize._minimize_cg(f,[0.99,0.99],jac=jacobian))\n",
    "    print(\"_______________________________________\")\n",
    "\n",
    "    print(\"Newton-CG\")\n",
    "    print('for x0 = [0,0]')\n",
    "    print(optimize._minimize_newtoncg(f,[0,0],jac=jacobian))\n",
    "    print(\"_______________________________________\")\n",
    "\n",
    "    print('for x0 = [0.99,0.99]')\n",
    "    print(optimize._minimize_newtoncg(f,[0.99,0.99],jac=jacobian))\n",
    "    print(\"_______________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-a6a5b06f",
   "language": "python",
   "display_name": "PyCharm (pythonProject1)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}